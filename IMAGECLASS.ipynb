{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1939c9fd-defe-4322-9c1d-12a2b01a69a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Setup and Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683016e-75e7-4ca6-bbbf-78494c7134a3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd770e6-dfac-4581-96d4-9183d72a192a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15210e69-d698-4bd5-acea-f147dd579445",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059196a-8a9b-4478-86ea-5f71159cdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c51cf-c17e-45c6-aaae-9bb42f588646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb1fb0-263c-4545-a171-e4351df6ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid OOM(Out Of Memory) error, set GPU to certain Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc2136-b15f-4e39-b1c7-a7f9b2b8186c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe6950-5720-4305-b7b6-bea864f0b6b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Remove dodgy images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94817bf7-3866-44af-815c-b105caa61bb8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d028b3-8617-4815-9a9c-cd06aebeb0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imghdr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8beaff-a020-4a5a-8cfe-08f5a9057f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63c6d5-357a-4e3f-a27a-dfa0a99706b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(data_directory_path,'happy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76168cc2-6ae2-42eb-83f6-cd415dea950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_extension = ['jpeg','jpg','bmp','png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae49cec-e8fe-4f17-ac24-96046990cddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(os.path.join('data','happy','smile.woman_.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031e57c-be51-437c-9c29-2552be0fbeb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image.shape #heght, width, color(if color = 3:Colored, else:b&w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f83d92-b6ed-475f-982f-a1be47827fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191ccb6-5060-4fec-9058-53266058d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_class in os.listdir(data_directory_path):\n",
    "    for image in os.listdir(os.path.join(data_directory_path, image_class)):\n",
    "        image_path = os.path.join(data_directory_path, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_extension:\n",
    "                print('Image not in ext list {}'.format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print('Issue with image {},{}'.format(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975006c-5ec2-4398-964b-8c6184165193",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bbb92-9312-4fef-ba7d-b23c6cc2a075",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6bd45-9370-4292-a60a-712a056a1038",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fce948-6cd1-49dd-8535-d49ef3e9c3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.data.Dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886733c5-b51f-4a4f-8ff1-472ca627cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7e520-b8b2-4441-9917-6b297b4598c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.utils.image_dataset_from_directory('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6d2c5-8c26-4fbd-91e6-b6638ca18734",
   "metadata": {},
   "source": [
    "Two classes are created one for happy people and one class for sad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548fdbd9-f2ae-4dfc-b4ea-035bb3417b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689337c4-62c0-4398-a17d-b27e44468786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get another batch from the iterator\n",
    "batch = data_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0428514-b240-47b9-b2ae-3771577f173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch) #Image Set and Label Set respectively will be batch, which can be accessed through indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf1b36-f5f3-4726-84ce-2805ac0d7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Images represented as numpy arrays\n",
    "#Image dataset is divided into batches of 32 images by keras\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f21d19-9d23-4154-9917-b6d78df21322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The labels are represented as 1 and 0.\n",
    "#Class 0 : Happy\n",
    "#Class 1 : Sad\n",
    "#For every iteration new batch 32 images is pushed throug the pipeline.\n",
    "batch[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ab060-a331-4f37-a0f3-b7d3817becfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img.astype(int)) \n",
    "    #Converts the image data type to int for display purposes.\n",
    "    #converts the pixel values of the image (img) to integers. \n",
    "    #This assumes that the original pixel values are already in a normalized range (e.g., 0-1) and multiplies them by 255 to scale them to the 0-255 range.\n",
    "    ax[idx].title.set_text(batch[1][idx]) #Here scaled_batch[1] represents the label set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab3d14-ba0d-4824-8043-e02d40bb99ad",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3c38c-3022-45d6-8ad3-5fafaf4893b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59e599-ba70-4047-9b90-1c537940cc1c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4adb1e9-a2e7-45ce-8c7e-fcdf878c95e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Scale Data = When we load the images, the values of each image is between 0 and 255. For optimizing we can scale the image's vale between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ab4bc-1ff6-4f79-9e5e-6341a6d8f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].min() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930076e9-dc0c-40ce-a6fe-cff4f3a77654",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x,y: (x/255, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4d1da-422b-49cc-b473-e767616e7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a901d81-365b-48da-97f7-a272d1f268a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_batch = next(scaled_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e646d47-3707-4932-a866-333adbeba26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, plot = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for indx, img in enumerate(scaled_batch[0][:4]):\n",
    "    plot[indx].imshow(img) #Here conversion to int is not possible as the scaling is done between 0 and 1.\n",
    "    plot[indx].title.set_text(scaled_batch[1][indx]) #Here scaled_batch[1] represents the label set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b5bd7-f07b-4c50-ad93-6c0097d851f4",
   "metadata": {},
   "source": [
    "##### Split data for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877f441-99b1-4dbc-bd33-671b1b76c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of data(number of batches)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de22f4-42b3-4a60-bd5e-310fd22a99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data)*.7) #70% of batches\n",
    "val_size = int(len(data)*.2) #20% of batches\n",
    "test_size = int(len(data)*.1)+1 #10% of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc872fd-6c6e-4f28-8eb1-a9195d69a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size + val_size + test_size #total number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648723d-16d0-49c7-855b-4abd6949c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the data according to the above mentioned sizes.\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size) #skip() method skips the alreay taken batches\n",
    "test = data.skip(train_size + val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03cf27-0e1f-47a4-b0cd-578c374fdaa7",
   "metadata": {},
   "source": [
    "##### Building Deep Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab332c42-9787-44ae-97f4-cd3c1932c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential # Keras Sequential Model \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten # Layers added to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056e5ac-f8f0-4e55-809e-458392054d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11de11-4196-44b6-a17c-b6add33f7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers are added one by one to the model in the sequential manner\n",
    "\n",
    "model.add(Conv2D(16,(3,3),1, activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(32,(3,3),1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(16,(3,3),1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) #Single Output Layer \n",
    "# Here the sigmoid activation brings the value between 0 and 1, which is used for detection of sad and happy people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19e8ad-3635-4948-baac-8c3c7f9d29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam',loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy']) #using optimazation module adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb17da-ed5f-4b65-a38a-4b3e7c624d65",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393c9c9-1d6e-41af-b2f2-0ca04b0bbc43",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ad2db-ef0e-4ddd-8da2-90c56e3458ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c592c7a-2cb8-485e-a36e-1ba252faf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ceb2f7-13b6-4b48-a95a-e85e91f0bde0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Component\n",
    "# Epochs specify the number of runs over the training data set. If it is 1, its going to be 1 run over the training dataset.\n",
    "hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488109f-4f51-4a95-b602-e1ece5de5bbb",
   "metadata": {},
   "source": [
    "##### Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1eb62-81d2-4815-8b7e-bde771c1a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d322c-3c52-4898-9505-61c64fceb21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a3dea-ba8e-46e0-8d32-7998a6201342",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d0ac9-4468-4be1-812e-36c03c3fceb9",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e347f24-ce8e-4a7b-b168-a8e65fe619f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing measures for this classification problem\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef00a61-ff8f-4966-a089-d4349afb0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instances for measures\n",
    "pre = Precision()\n",
    "rec = Recall()\n",
    "accu = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e6e76-cc4f-4561-939b-7901b9261ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaled_batch in test.as_numpy_iterator():\n",
    "    X,y = scaled_batch # X= set of images\n",
    "    yhat = model.predict(X) # yhat value will be either 0 or 1, its predicted, as of the images and that is done through sigmoid activation\n",
    "    pre.update_state(y, yhat) # Here the comparison is done between y and yhat value, where y stores the orginal label of the image and yhat is the newly predcted one.\n",
    "    rec.update_state(y, yhat)\n",
    "    accu.update_state(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406092f1-2901-4b72-b567-e44521d2be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision:{pre.result().numpy()}, Recall:{rec.result().numpy()}, Accuracy:{accu.result().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f7f93-8dff-463d-9837-ea983e62f84b",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c74ed56-a43e-4284-9b15-5658d65c9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('sad_image_test.jpg')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916271a-b6dd-410a-9230-badb3898d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the neural network every image must be (256px Hight,256px Width, 3 Chanells(RGB))\n",
    "resize = tf.image.resize(img, (256, 256))\n",
    "plt.imshow(resize.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7e0f0-8288-463d-90ae-466f679a898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing the image thorugh the neural network\n",
    "yhat = model.predict(np.expand_dims(resize/255, 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85656284-5156-414b-8f0b-afd2ce1aefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c94337-a159-48e6-a7c1-103d81f9b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "if yhat > 0.5 :\n",
    "    print(\"The preidicted image is SAD\")\n",
    "else:\n",
    "    print(\"The predicted image is HAPPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0591a331-d582-4d4c-a12e-3e8a7bd32e6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd128bb7-903c-4dff-951d-70879dd79830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9b2d9-8076-4ca7-9290-88729ad4858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join('Models','Happy_or_Sad_Image-Classification.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f925de-0a97-4d69-9eea-91a538e3126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "new_model = load_model(os.path.join('Models','Happy_or_Sad_Image-Classification.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a71e8b-64e8-4d0d-84aa-df361d3730a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the resized image through the model\n",
    "new_prediction = new_model.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140f15fb-d8ca-40b9-a5dd-8ec430c58321",
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_prediction > 0.5:\n",
    "    print(\"The predicted image is SAD\")\n",
    "else:\n",
    "    print(\"The predicted image is HAPPY\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageclassification",
   "language": "python",
   "name": "imageclassification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
